{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Abbreviations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), 'data'))\n",
    "\n",
    "import amharic_preprocessing_data\n",
    "\n",
    "abbreviations = amharic_preprocessing_data.abbreviations_dictionary_data\n",
    "\n",
    "stopwords = amharic_preprocessing_data.amharic_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "á‹¶/áˆ­: á‹¶áŠ­á‰°áˆ­\n",
      "á‹¶áˆ­: á‹¶áŠ­á‰°áˆ­\n",
      "áŒ /áˆš: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰´áˆ­\n",
      "áŒ /áˆšáŠ’áˆµá‰µáˆ­: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰µáˆ­\n",
      "áŒ /áˆšáŠ’áˆµá‰µáˆ­áŠá‰µ: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰µáˆ­áŠá‰µ\n",
      "áŒ /á/á‰¤á‰µ: áŒ á‰…áˆ‹á‹­ ááˆ­á‹µ á‰¤á‰µ\n",
      "áŒ/áˆ/á‰¤á‰µ: áŒá‹°áˆ«áˆ áˆáŠ­áˆ­ á‰¤á‰µ\n",
      "á/á‰¤á‰±: ááˆ­á‹µ á‰¤á‰±\n",
      "á/á‰¤á‰µ: ááˆ­á‹µ á‰¤á‰µ\n",
      "á•/á‰µ: á•áˆ¬á‹šá‹³áŠ•á‰µ\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(abbreviations.items())[:10]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['áˆ°áˆáŠ•', 'á‹ˆá‹²á‹«', 'áŒ‹áˆ­', 'á‹¨áŠ¥áˆ·', 'á‹ˆá‹­áˆµ', 'áŠ¥áŠ“áŠ•á‰°', 'áŠ¥áŠ”', 'áŠ¥á‹¨áŠ–áˆ­áŠ©', 'áŠ¥áŠ•á‹´á‰µ', 'á‰µ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "áˆá‹© á‹¨á‰°áˆáŒ¥áˆ® áŒˆá…á‰³ *****á‹¨áˆáˆµáˆ«á‰… áŠ ááˆªáŠ« á‹¨á‹áˆƒ áˆ›áˆ› áŒ®á‰„            \n",
      "@bobomaheder Global citizenáŠ•áŠá‰´ áŠ¥áŠ“ áŠ¢á‰µá‹®áŒµá‹«á‹ŠáŠá‰´ á‰°áˆá‰³á‰°á‹á‰¥áŠ áŠ á‹«á‹á‰áˆ á‹¨áˆšáˆ á‹­á‰³áŠ¨áˆá‰ á‰µ ğŸ˜‰\n",
      "RT @TechinEthiopia: áˆáˆ´á‰µáŠ• áŠ¨á‰ áŒáŠá‰µ !!! á‹¨áˆ°á‹ áˆáŒ… á‰ áˆá‹µáˆ­ áˆ²áˆ˜áˆ‹áˆˆáˆµ á‰³áˆ‹á‰… á‹¨áˆ˜áŠ•áˆáˆµ áŠ¥áˆ­áŠ«á‰³áŠ• áŠ¨áˆšá‹«áŒáŠ“á…á‰á‰µ á‰°áŒá‰£áˆ«á‰µ áˆ˜áŠ«áŠ¨áˆ áŠ áŠ•á‹± áˆˆá‰°á‰¸áŒˆáˆ¨ áˆ˜á‹µáˆ…áŠ•á¤ áˆˆá‹ˆáŒˆáŠ•áˆ áŠ áˆˆáŠá‰³ áˆ˜áˆ†áŠ• áˆ²á‰½áˆ áŠ¥áŠ•á‹°áˆ†áŠ á‰¥á‹™á‹á‰»á‰½áŠ•áŠ•â€¦\n",
      "áˆˆá‰…á‹³áˆšá‰³á‰½áˆ ğŸ’šğŸ’›â¤ áŠ áˆáŠ• á‰ áˆá‰µáˆ°áˆ©á‰µ áˆ›áŠ•áŠ›á‹áˆ áŠáŒˆáˆ­ áˆáˆ‰áŠ¥áŠá‰µáŠ“ áŠ¥áˆ­áŠ«á‰³ á‹­áˆ°áˆ›á‰½áˆ! á‹¨áŠ áˆáŠ•áŠá‰µ áˆ€á‹­áˆ -áˆˆáˆ˜áŠ•áˆáˆ³á‹Š á‹¨áŠ¥á‹á‰€á‰µ á‰¥áˆ­áˆƒáŠ• áˆ˜áˆ˜áˆªá‹« á‰ áŠ¤áŠ­áˆ€áˆ­á‰µ á‰¶áˆŒá‹¨á‰°á‹˜áŒ‹áŒ€ áˆ˜áŒ½áˆá áŠá‹á¢ https://t.co/RTi8kKKrd0 https://t.co/4YTikE0Amc\n",
      "@tesfamaryam21 40/60 áŠ®áŠ•á‹¶áˆšáŠ’á‹¨áˆ áŠ¨áˆ†áŠ áŠ¥áˆ˜áˆáˆ³áˆˆáˆ áŠ«áˆáˆ†áŠ áˆ˜áˆáˆ´áŠ• áŠ áˆ‹á‰£áŠ­áŠ•áˆ!\n",
      "@Jeberara1 áˆµáŠ•á‰³á‹¨áˆáˆ áˆ†áŠ áŠ¤áˆá‹«áˆµ á“áˆ­áˆ‹áˆ› áˆˆáˆ˜áŒá‰£á‰µ áˆáŠ• á‹«áŠ•áˆ³á‰¸á‹‹áˆ??? á‹°áŒáˆ áˆáˆˆá‰µ áˆ°á‹ á‰¥á‰» á‹­á‹ á“áˆ­á‰² á‹­áˆ˜áˆ°áˆ¨á‰³áˆ á‹«áˆˆáˆ… áˆ›áŠá‹? áˆ˜á‰¸áˆ á‰ áŠ¥áˆµáŠ­áŠ•á‹µáˆ­ áˆ‹á‹­ á‰ á‹¨áˆáŠ­áŠ’á‹«á‰± á‹±áˆ‹ á‹¨áˆ›á‹«áŠáˆ³ á‹¨áˆˆáˆ?! á‰ áˆ˜áˆ­áˆ… á‹¨áˆšáˆ˜áˆ« áŠ áŠ•á‹µ á…áŠ‘ áˆ°á‹ á‰¢áŠ–áˆ­ áŠ áˆá‰†áˆ áŠ áˆ‹áˆµá‰€áˆáŒ¥ áŠ áˆ‹á‰½áˆá‰µá¢\n",
      "@woldeyes_t Tesfaye áˆˆáŠ«áˆµ áŒ­á‰¥áˆ áˆˆá‰¥áˆ°áˆ½ á‹¨á•áˆ®áŒáˆ°áˆ­áŠ• áá‰¶ áˆˆáŒ¥áˆáŠ­ áŠ¥áˆáˆ á‹«áˆáŠ­ á‰£á‹³ áŠáŠ­ áŠ¥áˆáˆ­ á‰µáŠ•áˆ½\n",
      "á‰³áŠ…áˆ£áˆ¥ 10 á‰€áŠ• 2012 á‰ áˆáŒ£ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ á‰ áˆ˜áˆµáŒ…á‹¶á‰½áŠ“ áˆ±á‰†á‰½ áˆ‹á‹­ á‹¨á‹°áˆ¨áˆ°á‹áŠ• á‹¨áŠ¥áˆ³á‰µ á‰ƒáŒ áˆ á‰ áˆ›á‹áŒˆá‹ á‹¨áŠ¥áˆµáˆáˆáŠ“ áˆƒá‹­áˆ›áŠ–á‰µ á‰°áŠ¨á‰³á‹®á‰½ á‰ á‰£áˆ•áˆ­ á‹³áˆ­ áˆ áˆ‹áˆ›á‹Š áˆ áˆá áŠ¥á‹«áŠ«áˆ„á‹± á‹­áŒˆáŠ›áˆ‰á¡á¡ #Ethiopia #EthioMuslim https://t.co/bd5u0Rutn4\n",
      "áŠ áˆœáˆªáŠ« áˆ±áˆŒá‹­áˆ›áŠ’áŠ• á‰ áˆ˜áŒá‹°áˆ á‰€á‹­ áˆ˜áˆµáˆ˜áˆ©áŠ• áˆ³á‰³áˆá áŠ á‹­á‰€áˆ­áˆá¢ á‹œáŒá‰¿ áŠ áŒˆáˆªá‰±áŠ• áˆˆá‰€á‹ áŠ¥áŠ•á‹²á‹ˆáŒ¡ áŠ á‹á‹›áˆˆá‰½á¢ áŒ€áˆˆáˆ¶á‰¿ á‹°áŒáˆ restrain áŠ¥áŠ“ de-escalate áŠ¥á‹«áˆ‰ áˆ˜áˆˆáˆ›áˆ˜áŒ¥ áŒ€áˆáˆ¨á‹‹áˆá¢\n",
      "@haset_haset7 á‹­áˆ„á‹ áŠá‹ áŠ á‹­á‹°áˆ á‹¨áŠ¥á‹á‰€á‰µáˆ½ áŒ¥áŒ....á‰ áˆ°áˆš áˆ°áˆš áŠ¨áˆá‰µáŠ“áŒˆáˆª áˆˆáˆáŠ• á‰³áˆªáŠ­ áŠ á‰³áŠá‰¢áˆ....á‹°áˆ áˆ«áˆµáˆ½áŠ• áŠ á‰³áˆµáŒˆáˆá‰º\n",
      "\n",
      " Number of rows in the dataset: 17352\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join('data', 'dataset.csv'))\n",
    "\n",
    "for tweet in data['tweet'].head(10):\n",
    "    print(tweet.ljust(50))\n",
    "\n",
    "rows, columns = data.shape\n",
    "\n",
    "print(f\"\\n Number of rows in the dataset: {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Character-Level Mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char_level_missmatch(input_token):\n",
    "    \"\"\"\n",
    "    Normalizes character-level mismatches in Amharic text.\n",
    "\n",
    "    Parameters:\n",
    "        input_token (str): The Amharic text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    rep1 = re.sub('[áˆƒáŠ…áŠƒáˆáˆ“áŠ»]', 'áˆ€', input_token)\n",
    "    rep2 = re.sub('[áˆ‘áŠá‹…]', 'áˆ', rep1)\n",
    "    rep3 = re.sub('[áŠ‚áˆ’áŠº]', 'áˆ‚', rep2)\n",
    "    rep4 = re.sub('[áŠŒáˆ”á‹„]', 'áˆ„', rep3)\n",
    "    rep5 = re.sub('[áˆ•áŠ…]', 'áˆ…', rep4)\n",
    "    rep6 = re.sub('[áŠ†áˆ–áŠ¾]', 'áˆ†', rep5)\n",
    "    rep7 = re.sub('[áˆ ]', 'áˆ°', rep6)\n",
    "    rep8 = re.sub('[áˆ¡]', 'áˆ±', rep7)\n",
    "    rep9 = re.sub('[áˆ¢]', 'áˆ²', rep8)\n",
    "    rep10 = re.sub('[áˆ£]', 'áˆ³', rep9)\n",
    "    rep11 = re.sub('[áˆ¤]', 'áˆ´', rep10)\n",
    "    rep12 = re.sub('[áˆ¥]', 'áˆµ', rep11)\n",
    "    rep13 = re.sub('[áˆ¦]', 'áˆ¶', rep12)\n",
    "    rep14 = re.sub('[á‹“áŠ£á‹]', 'áŠ ', rep13)\n",
    "    rep15 = re.sub('[á‹‘]', 'áŠ¡', rep14)\n",
    "    rep16 = re.sub('[á‹’]', 'áŠ¢', rep15)\n",
    "    rep17 = re.sub('[á‹”]', 'áŠ¤', rep16)\n",
    "    rep18 = re.sub('[á‹•]', 'áŠ¥', rep17)\n",
    "    rep19 = re.sub('[á‹–]', 'áŠ¦', rep18)\n",
    "    rep20 = re.sub('[áŒ¸]', 'á€', rep19)\n",
    "    rep21 = re.sub('[áŒ¹]', 'á', rep20)\n",
    "    rep22 = re.sub('[áŒº]', 'á‚', rep21)\n",
    "    rep23 = re.sub('[áŒ»]', 'áƒ', rep22)\n",
    "    rep24 = re.sub('[áŒ¼]', 'á„', rep23)\n",
    "    rep25 = re.sub('[áŒ½]', 'á…', rep24)\n",
    "    rep26 = re.sub('[áŒ¾]', 'á†', rep25)\n",
    "    rep27 = re.sub('(áˆ‰[á‹‹áŠ ])', 'áˆ', rep26)\n",
    "    rep28 = re.sub('(áˆ™[á‹‹áŠ ])', 'áˆŸ', rep27)\n",
    "    rep29 = re.sub('(á‰±[á‹‹áŠ ])', 'á‰·', rep28)\n",
    "    rep30 = re.sub('(áˆ©[á‹‹áŠ ])', 'áˆ¯', rep29)\n",
    "    rep31 = re.sub('(áˆ±[á‹‹áŠ ])', 'áˆ·', rep30)\n",
    "    rep32 = re.sub('(áˆ¹[á‹‹áŠ ])', 'áˆ¿', rep31)\n",
    "    rep33 = re.sub('(á‰[á‹‹áŠ ])', 'á‰‹', rep32)\n",
    "    rep34 = re.sub('(á‰¡[á‹‹áŠ ])', 'á‰§', rep33)\n",
    "    rep35 = re.sub('(á‰¹[á‹‹áŠ ])', 'á‰¿', rep34)\n",
    "    rep36 = re.sub('(áˆ[á‹‹áŠ ])', 'áŠ‹', rep35)\n",
    "    rep37 = re.sub('(áŠ‘[á‹‹áŠ ])', 'áŠ—', rep36)\n",
    "    rep38 = re.sub('(áŠ™[á‹‹áŠ ])', 'áŠŸ', rep37)\n",
    "    rep39 = re.sub('(áŠ©[á‹‹áŠ ])', 'áŠ³', rep38)\n",
    "    rep40 = re.sub('(á‹™[á‹‹áŠ ])', 'á‹Ÿ', rep39)\n",
    "    rep41 = re.sub('(áŒ‰[á‹‹áŠ ])', 'áŒ“', rep40)\n",
    "    rep42 = re.sub('(á‹°[á‹‹áŠ ])', 'á‹·', rep41)\n",
    "    rep43 = re.sub('(áŒ¡[á‹‹áŠ ])', 'áŒ§', rep42)\n",
    "    rep44 = re.sub('(áŒ©[á‹‹áŠ ])', 'áŒ¯', rep43)\n",
    "    rep45 = re.sub('(áŒ¹[á‹‹áŠ ])', 'áŒ¿', rep44)\n",
    "    rep46 = re.sub('(á‰[á‹‹áŠ ])', 'á', rep45)\n",
    "    rep47 = re.sub('[á‰Š]', 'á‰', rep46)\n",
    "    rep48 = re.sub('[áŠµ]', 'áŠ©', rep47)\n",
    "    return rep48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Short Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_short_form(input_short_word):\n",
    "    \"\"\"\n",
    "    Expands abbreviations in Amharic text using a provided dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        input_short_word (str): The word to expand.\n",
    "        short_form_dict (dict): A dictionary with abbreviations as keys and their expansions as values.\n",
    "\n",
    "    Returns:\n",
    "        str: The expanded word if found in the dictionary, otherwise the original word.\n",
    "    \"\"\"\n",
    "    return abbreviations.get(input_short_word, input_short_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english(text):\n",
    "    \"\"\"\n",
    "    Removes English characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with English characters removed.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'[a-zA-Z]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_and_special_chars(text): \n",
    "    \"\"\"\n",
    "    Removes punctuation and special characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with punctuation and special characters removed.\n",
    "    \"\"\"\n",
    "    normalized_text = re.sub(r'[!@#\\$%\\^\\Â«\\Â»&\\*\\(\\)â€¦\\[\\]\\{\\};â€œâ€â€ºâ€™â€˜\"\\':,.\\â€¹/\\<\\>\\?\\\\|\\`\\Â´~\\-=+\\á¡á¢á¤;á¦á¥á§á¨á á£_]', '', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing ASCII Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ascii_and_numbers(text_input):\n",
    "    \"\"\"\n",
    "    Removes all ASCII characters, English numbers, and Amharic/Arabic numbers from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text_input (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with ASCII characters and numbers removed.\n",
    "    \n",
    "    Notes:\n",
    "        - ASCII characters include all English letters (A-Z, a-z) and digits (0-9).\n",
    "        - Amharic/Arabic numbers are removed based on their Unicode range (U+1369 to U+137C).\n",
    "        - This function is helpful for preprocessing text where only specific non-numeric and non-ASCII \n",
    "          characters (e.g., Amharic script) are required.\n",
    "    \"\"\"\n",
    "    rm_num_and_ascii = re.sub('[A-Za-z0-9]', '', text_input)\n",
    "    \n",
    "    cleaned_text = re.sub('[\\u1369-\\u137C]+', '', rm_num_and_ascii)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace from the start and end of the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The trimmed text with no leading or trailing whitespace.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs, including shortened links like 'https://t.co/...', from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with URLs removed.\n",
    "    \"\"\"\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    cleaned_text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "        stopwords (list): A list of stopwords.\n",
    "\n",
    "    Returns:\n",
    "        str: The text without stopwords.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Removes emojis from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with emojis removed.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # emotions\n",
    "                                 \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                 \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                 \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                 \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                 \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                 \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                 \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                 \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                                 \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Amharic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amharic_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the given Amharic text by applying various preprocessing steps.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Amharic text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after applying all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    text = remove_english(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_ascii_and_numbers(text)\n",
    "    text = remove_punc_and_special_chars(text)\n",
    "    text = normalize_char_level_missmatch(text)\n",
    "    text = expand_short_form(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Cleaning Function to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet'].apply(clean_amharic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 cleaned tweets:\n",
      "0. áˆá‹© á‹¨á‰°áˆáŒ¥áˆ® áŒˆá…á‰³ á‹¨áˆáˆµáˆ«á‰… áŠ ááˆªáŠ« á‹¨á‹áˆ€ áˆ›áˆ› áŒ®á‰„                 \n",
      "1. áŠ¢á‰µá‹®áŒµá‹«á‹ŠáŠá‰´ á‰°áˆá‰³á‰°á‹á‰¥áŠ áŠ á‹«á‹á‰áˆ á‹¨áˆšáˆ á‹­á‰³áŠ¨áˆá‰ á‰µ                 \n",
      "2. áˆ€áˆ´á‰µáŠ• áŠ¨á‰ áŒáŠá‰µ á‹¨áˆ°á‹ áˆáŒ… á‰ áˆá‹µáˆ­ áˆ²áˆ˜áˆ‹áˆˆáˆµ á‰³áˆ‹á‰… á‹¨áˆ˜áŠ•áˆáˆµ áŠ¥áˆ­áŠ«á‰³áŠ• áŠ¨áˆšá‹«áŒáŠ“á…á‰á‰µ á‰°áŒá‰£áˆ«á‰µ áŠ áŠ•á‹± áˆˆá‰°á‰¸áŒˆáˆ¨ áˆ˜á‹µáˆ…áŠ• áˆˆá‹ˆáŒˆáŠ•áˆ áŠ áˆˆáŠá‰³ áˆ˜áˆ†áŠ• áˆ²á‰½áˆ á‰¥á‹™á‹á‰»á‰½áŠ•áŠ•\n",
      "3. áˆˆá‰…á‹³áˆšá‰³á‰½áˆ á‰ áˆá‰µáˆ°áˆ©á‰µ áˆ›áŠ•áŠ›á‹áˆ áŠáŒˆáˆ­ áˆáˆ‰áŠ¥áŠá‰µáŠ“ áŠ¥áˆ­áŠ«á‰³ á‹­áˆ°áˆ›á‰½áˆ á‹¨áŠ áˆáŠ•áŠá‰µ áˆ€á‹­áˆ áˆˆáˆ˜áŠ•áˆáˆ³á‹Š á‹¨áŠ¥á‹á‰€á‰µ á‰¥áˆ­áˆ€áŠ• áˆ˜áˆ˜áˆªá‹« á‰ áŠ¤áŠ­áˆ€áˆ­á‰µ á‰¶áˆŒá‹¨á‰°á‹˜áŒ‹áŒ€ áˆ˜á…áˆ€á\n",
      "4. áŠ®áŠ•á‹¶áˆšáŠ’á‹¨áˆ áŠ¨áˆ†áŠ áŠ¥áˆ˜áˆáˆ³áˆˆáˆ áŠ«áˆáˆ†áŠ áˆ˜áˆáˆ´áŠ• áŠ áˆ‹á‰£áŠ­áŠ•áˆ               \n",
      "5. áˆµáŠ•á‰³á‹¨áˆáˆ áŠ¤áˆá‹«áˆµ á“áˆ­áˆ‹áˆ› áˆˆáˆ˜áŒá‰£á‰µ á‹«áŠ•áˆ³á‰¸á‹‹áˆ á‹°áŒáˆ áˆáˆˆá‰µ áˆ°á‹ á‹­á‹ á“áˆ­á‰² á‹­áˆ˜áˆ°áˆ¨á‰³áˆ á‹«áˆˆáˆ… áˆ›áŠá‹ áˆ˜á‰¸áˆ á‰ áŠ¥áˆµáŠ­áŠ•á‹µáˆ­ á‰ á‹¨áˆáŠ­áŠ’á‹«á‰± á‹±áˆ‹ á‹¨áˆ›á‹«áŠáˆ³ á‹¨áˆˆáˆ á‰ áˆ˜áˆ­áˆ… á‹¨áˆšáˆ˜áˆ« á…áŠ‘ áˆ°á‹ á‰¢áŠ–áˆ­ áŠ áˆá‰†áˆ áŠ áˆ‹áˆµá‰€áˆáŒ¥ áŠ áˆ‹á‰½áˆá‰µ\n",
      "6. áˆˆáŠ«áˆµ áŒ­á‰¥áˆ áˆˆá‰¥áˆ°áˆ½ á‹¨á•áˆ®áŒáˆ°áˆ­áŠ• áá‰¶ áˆˆáŒ¥áˆáŠ­ áŠ¥áˆáˆ á‹«áˆáŠ­ á‰£á‹³ áŠáŠ­ áŠ¥áˆáˆ­    \n",
      "7. á‰³áˆ€áˆ³áˆµ á‰€áŠ• á‰ áˆáŒ£ áŠ¨á‰°áˆ› áŠ áˆµá‰°á‹³á‹°áˆ­ á‰ áˆ˜áˆµáŒ…á‹¶á‰½áŠ“ áˆ±á‰†á‰½ á‹¨á‹°áˆ¨áˆ°á‹áŠ• á‹¨áŠ¥áˆ³á‰µ á‰ƒáŒ áˆ á‰ áˆ›á‹áŒˆá‹ á‹¨áŠ¥áˆµáˆáˆáŠ“ áˆ€á‹­áˆ›áŠ–á‰µ á‰°áŠ¨á‰³á‹®á‰½ á‰ á‰£áˆ…áˆ­ áˆ°áˆ‹áˆ›á‹Š áˆ°áˆá áŠ¥á‹«áŠ«áˆ„á‹± á‹­áŒˆáŠ›áˆ‰\n",
      "8. áŠ áˆœáˆªáŠ« áˆ±áˆŒá‹­áˆ›áŠ’áŠ• á‰ áˆ˜áŒá‹°áˆ á‰€á‹­ áˆ˜áˆµáˆ˜áˆ©áŠ• áˆ³á‰³áˆá áŠ á‹­á‰€áˆ­áˆ á‹œáŒá‰¿ áŠ áŒˆáˆªá‰±áŠ• áˆˆá‰€á‹ áŠ¥áŠ•á‹²á‹ˆáŒ¡ áŠ á‹á‹›áˆˆá‰½ áŒ€áˆˆáˆ¶á‰¿ á‹°áŒáˆ áŠ¥á‹«áˆ‰ áˆ˜áˆˆáˆ›áˆ˜áŒ¥ áŒ€áˆáˆ¨á‹‹áˆ\n",
      "9. á‹­áˆ„á‹ áŠ á‹­á‹°áˆ á‹¨áŠ¥á‹á‰€á‰µáˆ½ áŒ¥áŒá‰ áˆ°áˆš áˆ°áˆš áŠ¨áˆá‰µáŠ“áŒˆáˆª á‰³áˆªáŠ­ áŠ á‰³áŠá‰¢áˆá‹°áˆ áˆ«áˆµáˆ½áŠ• áŠ á‰³áˆµáŒˆáˆá‰º\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 10 cleaned tweets:\")\n",
    "for index, tweet in enumerate(data['cleaned_tweet'].head(10)):\n",
    "    print(f\"{index}. {tweet.ljust(50)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
