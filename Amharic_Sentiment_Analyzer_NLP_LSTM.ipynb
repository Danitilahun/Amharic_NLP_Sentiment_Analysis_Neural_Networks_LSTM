{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.0/9.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.1/9.7 MB 491.5 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/9.7 MB 585.1 kB/s eta 0:00:17\n",
      "    --------------------------------------- 0.2/9.7 MB 654.6 kB/s eta 0:00:15\n",
      "    --------------------------------------- 0.2/9.7 MB 860.2 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/9.7 MB 951.8 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.5/9.7 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/9.7 MB 1.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/9.7 MB 1.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 1.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/9.7 MB 2.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.5/9.7 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.2/9.7 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.3/9.7 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 450.7/450.7 kB 29.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 303.8/303.8 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 43.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 38.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.3\n",
      "    Uninstalling huggingface-hub-0.23.3:\n",
      "      Successfully uninstalled huggingface-hub-0.23.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --default-timeout=100 pandas transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Abbreviations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), 'data'))\n",
    "\n",
    "import amharic_preprocessing_data\n",
    "\n",
    "abbreviations = amharic_preprocessing_data.abbreviations_dictionary_data\n",
    "\n",
    "stopwords = amharic_preprocessing_data.amharic_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "á‹¶/áˆ­: á‹¶áŠ­á‰°áˆ­\n",
      "á‹¶áˆ­: á‹¶áŠ­á‰°áˆ­\n",
      "áŒ /áˆš: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰´áˆ­\n",
      "áŒ /áˆšáŠ’áˆµá‰µáˆ­: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰µáˆ­\n",
      "áŒ /áˆšáŠ’áˆµá‰µáˆ­áŠá‰µ: áŒ á‰…áˆ‹á‹­ áˆšáŠ’áˆµá‰µáˆ­áŠá‰µ\n",
      "áŒ /á/á‰¤á‰µ: áŒ á‰…áˆ‹á‹­ ááˆ­á‹µ á‰¤á‰µ\n",
      "áŒ/áˆ/á‰¤á‰µ: áŒá‹°áˆ«áˆ áˆáŠ­áˆ­ á‰¤á‰µ\n",
      "á/á‰¤á‰±: ááˆ­á‹µ á‰¤á‰±\n",
      "á/á‰¤á‰µ: ááˆ­á‹µ á‰¤á‰µ\n",
      "á•/á‰µ: á•áˆ¬á‹šá‹³áŠ•á‰µ\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(abbreviations.items())[:10]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['áˆ°áˆáŠ•', 'á‹ˆá‹²á‹«', 'áŒ‹áˆ­', 'á‹¨áŠ¥áˆ·', 'á‹ˆá‹­áˆµ', 'áŠ¥áŠ“áŠ•á‰°', 'áŠ¥áŠ”', 'áŠ¥á‹¨áŠ–áˆ­áŠ©', 'áŠ¥áŠ•á‹´á‰µ', 'á‰µ']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "áˆá‹© á‹¨á‰°áˆáŒ¥áˆ® áŒˆá…á‰³ *****á‹¨áˆáˆµáˆ«á‰… áŠ ááˆªáŠ« á‹¨á‹áˆƒ áˆ›áˆ› áŒ®á‰„            \n",
      "@bobomaheder Global citizenáŠ•áŠá‰´ áŠ¥áŠ“ áŠ¢á‰µá‹®áŒµá‹«á‹ŠáŠá‰´ á‰°áˆá‰³á‰°á‹á‰¥áŠ áŠ á‹«á‹á‰áˆ á‹¨áˆšáˆ á‹­á‰³áŠ¨áˆá‰ á‰µ ğŸ˜‰\n",
      "RT @TechinEthiopia: áˆáˆ´á‰µáŠ• áŠ¨á‰ áŒáŠá‰µ !!! á‹¨áˆ°á‹ áˆáŒ… á‰ áˆá‹µáˆ­ áˆ²áˆ˜áˆ‹áˆˆáˆµ á‰³áˆ‹á‰… á‹¨áˆ˜áŠ•áˆáˆµ áŠ¥áˆ­áŠ«á‰³áŠ• áŠ¨áˆšá‹«áŒáŠ“á…á‰á‰µ á‰°áŒá‰£áˆ«á‰µ áˆ˜áŠ«áŠ¨áˆ áŠ áŠ•á‹± áˆˆá‰°á‰¸áŒˆáˆ¨ áˆ˜á‹µáˆ…áŠ•á¤ áˆˆá‹ˆáŒˆáŠ•áˆ áŠ áˆˆáŠá‰³ áˆ˜áˆ†áŠ• áˆ²á‰½áˆ áŠ¥áŠ•á‹°áˆ†áŠ á‰¥á‹™á‹á‰»á‰½áŠ•áŠ•â€¦\n",
      "áˆˆá‰…á‹³áˆšá‰³á‰½áˆ ğŸ’šğŸ’›â¤ áŠ áˆáŠ• á‰ áˆá‰µáˆ°áˆ©á‰µ áˆ›áŠ•áŠ›á‹áˆ áŠáŒˆáˆ­ áˆáˆ‰áŠ¥áŠá‰µáŠ“ áŠ¥áˆ­áŠ«á‰³ á‹­áˆ°áˆ›á‰½áˆ! á‹¨áŠ áˆáŠ•áŠá‰µ áˆ€á‹­áˆ -áˆˆáˆ˜áŠ•áˆáˆ³á‹Š á‹¨áŠ¥á‹á‰€á‰µ á‰¥áˆ­áˆƒáŠ• áˆ˜áˆ˜áˆªá‹« á‰ áŠ¤áŠ­áˆ€áˆ­á‰µ á‰¶áˆŒá‹¨á‰°á‹˜áŒ‹áŒ€ áˆ˜áŒ½áˆá áŠá‹á¢ https://t.co/RTi8kKKrd0 https://t.co/4YTikE0Amc\n",
      "@tesfamaryam21 40/60 áŠ®áŠ•á‹¶áˆšáŠ’á‹¨áˆ áŠ¨áˆ†áŠ áŠ¥áˆ˜áˆáˆ³áˆˆáˆ áŠ«áˆáˆ†áŠ áˆ˜áˆáˆ´áŠ• áŠ áˆ‹á‰£áŠ­áŠ•áˆ!\n",
      "@Jeberara1 áˆµáŠ•á‰³á‹¨áˆáˆ áˆ†áŠ áŠ¤áˆá‹«áˆµ á“áˆ­áˆ‹áˆ› áˆˆáˆ˜áŒá‰£á‰µ áˆáŠ• á‹«áŠ•áˆ³á‰¸á‹‹áˆ??? á‹°áŒáˆ áˆáˆˆá‰µ áˆ°á‹ á‰¥á‰» á‹­á‹ á“áˆ­á‰² á‹­áˆ˜áˆ°áˆ¨á‰³áˆ á‹«áˆˆáˆ… áˆ›áŠá‹? áˆ˜á‰¸áˆ á‰ áŠ¥áˆµáŠ­áŠ•á‹µáˆ­ áˆ‹á‹­ á‰ á‹¨áˆáŠ­áŠ’á‹«á‰± á‹±áˆ‹ á‹¨áˆ›á‹«áŠáˆ³ á‹¨áˆˆáˆ?! á‰ áˆ˜áˆ­áˆ… á‹¨áˆšáˆ˜áˆ« áŠ áŠ•á‹µ á…áŠ‘ áˆ°á‹ á‰¢áŠ–áˆ­ áŠ áˆá‰†áˆ áŠ áˆ‹áˆµá‰€áˆáŒ¥ áŠ áˆ‹á‰½áˆá‰µá¢\n",
      "@woldeyes_t Tesfaye áˆˆáŠ«áˆµ áŒ­á‰¥áˆ áˆˆá‰¥áˆ°áˆ½ á‹¨á•áˆ®áŒáˆ°áˆ­áŠ• áá‰¶ áˆˆáŒ¥áˆáŠ­ áŠ¥áˆáˆ á‹«áˆáŠ­ á‰£á‹³ áŠáŠ­ áŠ¥áˆáˆ­ á‰µáŠ•áˆ½\n",
      "áŠ áˆœáˆªáŠ« áˆ±áˆŒá‹­áˆ›áŠ’áŠ• á‰ áˆ˜áŒá‹°áˆ á‰€á‹­ áˆ˜áˆµáˆ˜áˆ©áŠ• áˆ³á‰³áˆá áŠ á‹­á‰€áˆ­áˆá¢ á‹œáŒá‰¿ áŠ áŒˆáˆªá‰±áŠ• áˆˆá‰€á‹ áŠ¥áŠ•á‹²á‹ˆáŒ¡ áŠ á‹á‹›áˆˆá‰½á¢ áŒ€áˆˆáˆ¶á‰¿ á‹°áŒáˆ restrain áŠ¥áŠ“ de-escalate áŠ¥á‹«áˆ‰ áˆ˜áˆˆáˆ›áˆ˜áŒ¥ áŒ€áˆáˆ¨á‹‹áˆá¢\n",
      "@haset_haset7 á‹­áˆ„á‹ áŠá‹ áŠ á‹­á‹°áˆ á‹¨áŠ¥á‹á‰€á‰µáˆ½ áŒ¥áŒ....á‰ áˆ°áˆš áˆ°áˆš áŠ¨áˆá‰µáŠ“áŒˆáˆª áˆˆáˆáŠ• á‰³áˆªáŠ­ áŠ á‰³áŠá‰¢áˆ....á‹°áˆ áˆ«áˆµáˆ½áŠ• áŠ á‰³áˆµáŒˆáˆá‰º\n",
      "@BirrkanF @abrshel @melak_abera á‹¨áˆšá‰£áŠ­áŠ• áŒŠá‹œ áˆ˜áŠ–áˆ­ á‹¨áˆˆá‰ á‰µáˆá¢\n",
      "á‹³á‹­ á‹ˆá‹° áˆ¥áˆ«! ğŸ˜œ\n",
      "\n",
      " Number of rows in the dataset: 16722\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join('data', 'dataset.csv'))\n",
    "\n",
    "for tweet in data['tweet'].head(10):\n",
    "    print(tweet.ljust(50))\n",
    "\n",
    "rows, columns = data.shape\n",
    "\n",
    "print(f\"\\n Number of rows in the dataset: {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Character-Level Mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char_level_missmatch(input_token):\n",
    "    \"\"\"\n",
    "    Normalizes character-level mismatches in Amharic text.\n",
    "\n",
    "    Parameters:\n",
    "        input_token (str): The Amharic text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    rep1 = re.sub('[áˆƒáŠ…áŠƒáˆáˆ“áŠ»]', 'áˆ€', input_token)\n",
    "    rep2 = re.sub('[áˆ‘áŠá‹…]', 'áˆ', rep1)\n",
    "    rep3 = re.sub('[áŠ‚áˆ’áŠº]', 'áˆ‚', rep2)\n",
    "    rep4 = re.sub('[áŠŒáˆ”á‹„]', 'áˆ„', rep3)\n",
    "    rep5 = re.sub('[áˆ•áŠ…]', 'áˆ…', rep4)\n",
    "    rep6 = re.sub('[áŠ†áˆ–áŠ¾]', 'áˆ†', rep5)\n",
    "    rep7 = re.sub('[áˆ ]', 'áˆ°', rep6)\n",
    "    rep8 = re.sub('[áˆ¡]', 'áˆ±', rep7)\n",
    "    rep9 = re.sub('[áˆ¢]', 'áˆ²', rep8)\n",
    "    rep10 = re.sub('[áˆ£]', 'áˆ³', rep9)\n",
    "    rep11 = re.sub('[áˆ¤]', 'áˆ´', rep10)\n",
    "    rep12 = re.sub('[áˆ¥]', 'áˆµ', rep11)\n",
    "    rep13 = re.sub('[áˆ¦]', 'áˆ¶', rep12)\n",
    "    rep14 = re.sub('[á‹“áŠ£á‹]', 'áŠ ', rep13)\n",
    "    rep15 = re.sub('[á‹‘]', 'áŠ¡', rep14)\n",
    "    rep16 = re.sub('[á‹’]', 'áŠ¢', rep15)\n",
    "    rep17 = re.sub('[á‹”]', 'áŠ¤', rep16)\n",
    "    rep18 = re.sub('[á‹•]', 'áŠ¥', rep17)\n",
    "    rep19 = re.sub('[á‹–]', 'áŠ¦', rep18)\n",
    "    rep20 = re.sub('[áŒ¸]', 'á€', rep19)\n",
    "    rep21 = re.sub('[áŒ¹]', 'á', rep20)\n",
    "    rep22 = re.sub('[áŒº]', 'á‚', rep21)\n",
    "    rep23 = re.sub('[áŒ»]', 'áƒ', rep22)\n",
    "    rep24 = re.sub('[áŒ¼]', 'á„', rep23)\n",
    "    rep25 = re.sub('[áŒ½]', 'á…', rep24)\n",
    "    rep26 = re.sub('[áŒ¾]', 'á†', rep25)\n",
    "    rep27 = re.sub('(áˆ‰[á‹‹áŠ ])', 'áˆ', rep26)\n",
    "    rep28 = re.sub('(áˆ™[á‹‹áŠ ])', 'áˆŸ', rep27)\n",
    "    rep29 = re.sub('(á‰±[á‹‹áŠ ])', 'á‰·', rep28)\n",
    "    rep30 = re.sub('(áˆ©[á‹‹áŠ ])', 'áˆ¯', rep29)\n",
    "    rep31 = re.sub('(áˆ±[á‹‹áŠ ])', 'áˆ·', rep30)\n",
    "    rep32 = re.sub('(áˆ¹[á‹‹áŠ ])', 'áˆ¿', rep31)\n",
    "    rep33 = re.sub('(á‰[á‹‹áŠ ])', 'á‰‹', rep32)\n",
    "    rep34 = re.sub('(á‰¡[á‹‹áŠ ])', 'á‰§', rep33)\n",
    "    rep35 = re.sub('(á‰¹[á‹‹áŠ ])', 'á‰¿', rep34)\n",
    "    rep36 = re.sub('(áˆ[á‹‹áŠ ])', 'áŠ‹', rep35)\n",
    "    rep37 = re.sub('(áŠ‘[á‹‹áŠ ])', 'áŠ—', rep36)\n",
    "    rep38 = re.sub('(áŠ™[á‹‹áŠ ])', 'áŠŸ', rep37)\n",
    "    rep39 = re.sub('(áŠ©[á‹‹áŠ ])', 'áŠ³', rep38)\n",
    "    rep40 = re.sub('(á‹™[á‹‹áŠ ])', 'á‹Ÿ', rep39)\n",
    "    rep41 = re.sub('(áŒ‰[á‹‹áŠ ])', 'áŒ“', rep40)\n",
    "    rep42 = re.sub('(á‹°[á‹‹áŠ ])', 'á‹·', rep41)\n",
    "    rep43 = re.sub('(áŒ¡[á‹‹áŠ ])', 'áŒ§', rep42)\n",
    "    rep44 = re.sub('(áŒ©[á‹‹áŠ ])', 'áŒ¯', rep43)\n",
    "    rep45 = re.sub('(áŒ¹[á‹‹áŠ ])', 'áŒ¿', rep44)\n",
    "    rep46 = re.sub('(á‰[á‹‹áŠ ])', 'á', rep45)\n",
    "    rep47 = re.sub('[á‰Š]', 'á‰', rep46)\n",
    "    rep48 = re.sub('[áŠµ]', 'áŠ©', rep47)\n",
    "    return rep48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Short Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_short_form(input_short_word):\n",
    "    \"\"\"\n",
    "    Expands abbreviations in Amharic text using a provided dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        input_short_word (str): The word to expand.\n",
    "        short_form_dict (dict): A dictionary with abbreviations as keys and their expansions as values.\n",
    "\n",
    "    Returns:\n",
    "        str: The expanded word if found in the dictionary, otherwise the original word.\n",
    "    \"\"\"\n",
    "    return abbreviations.get(input_short_word, input_short_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english(text):\n",
    "    \"\"\"\n",
    "    Removes English characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with English characters removed.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'[a-zA-Z]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_and_special_chars(text): \n",
    "    \"\"\"\n",
    "    Removes punctuation and special characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with punctuation and special characters removed.\n",
    "    \"\"\"\n",
    "    normalized_text = re.sub(r'[!@#\\$%\\^\\Â«\\Â»&\\*\\(\\)â€¦\\[\\]\\{\\};â€œâ€â€ºâ€™â€˜\"\\':,.\\â€¹/\\<\\>\\?\\\\|\\`\\Â´~\\-=+\\á¡á¢á¤;á¦á¥á§á¨á á£_]', '', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing ASCII Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ascii_and_numbers(text_input):\n",
    "    \"\"\"\n",
    "    Removes all ASCII characters, English numbers, and Amharic/Arabic numbers from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text_input (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with ASCII characters and numbers removed.\n",
    "    \n",
    "    Notes:\n",
    "        - ASCII characters include all English letters (A-Z, a-z) and digits (0-9).\n",
    "        - Amharic/Arabic numbers are removed based on their Unicode range (U+1369 to U+137C).\n",
    "        - This function is helpful for preprocessing text where only specific non-numeric and non-ASCII \n",
    "          characters (e.g., Amharic script) are required.\n",
    "    \"\"\"\n",
    "    rm_num_and_ascii = re.sub('[A-Za-z0-9]', '', text_input)\n",
    "    \n",
    "    cleaned_text = re.sub('[\\u1369-\\u137C]+', '', rm_num_and_ascii)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace from the start and end of the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The trimmed text with no leading or trailing whitespace.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs, including shortened links like 'https://t.co/...', from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with URLs removed.\n",
    "    \"\"\"\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    cleaned_text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "        stopwords (list): A list of stopwords.\n",
    "\n",
    "    Returns:\n",
    "        str: The text without stopwords.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Removes emojis from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with emojis removed.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # emotions\n",
    "                                 \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                 \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                 \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                 \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                 \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                 \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                 \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                 \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                                 \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Amharic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amharic_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the given Amharic text by applying various preprocessing steps.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Amharic text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after applying all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    text = remove_english(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_ascii_and_numbers(text)\n",
    "    text = remove_punc_and_special_chars(text)\n",
    "    text = normalize_char_level_missmatch(text)\n",
    "    text = expand_short_form(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Cleaning Function to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet'].apply(clean_amharic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 cleaned tweets:\n",
      "0. áˆá‹© á‹¨á‰°áˆáŒ¥áˆ® áŒˆá…á‰³ á‹¨áˆáˆµáˆ«á‰… áŠ ááˆªáŠ« á‹¨á‹áˆ€ áˆ›áˆ› áŒ®á‰„                 \n",
      "1. áŠ¢á‰µá‹®áŒµá‹«á‹ŠáŠá‰´ á‰°áˆá‰³á‰°á‹á‰¥áŠ áŠ á‹«á‹á‰áˆ á‹¨áˆšáˆ á‹­á‰³áŠ¨áˆá‰ á‰µ                 \n",
      "2. áˆ€áˆ´á‰µáŠ• áŠ¨á‰ áŒáŠá‰µ á‹¨áˆ°á‹ áˆáŒ… á‰ áˆá‹µáˆ­ áˆ²áˆ˜áˆ‹áˆˆáˆµ á‰³áˆ‹á‰… á‹¨áˆ˜áŠ•áˆáˆµ áŠ¥áˆ­áŠ«á‰³áŠ• áŠ¨áˆšá‹«áŒáŠ“á…á‰á‰µ á‰°áŒá‰£áˆ«á‰µ áŠ áŠ•á‹± áˆˆá‰°á‰¸áŒˆáˆ¨ áˆ˜á‹µáˆ…áŠ• áˆˆá‹ˆáŒˆáŠ•áˆ áŠ áˆˆáŠá‰³ áˆ˜áˆ†áŠ• áˆ²á‰½áˆ á‰¥á‹™á‹á‰»á‰½áŠ•áŠ•\n",
      "3. áˆˆá‰…á‹³áˆšá‰³á‰½áˆ á‰ áˆá‰µáˆ°áˆ©á‰µ áˆ›áŠ•áŠ›á‹áˆ áŠáŒˆáˆ­ áˆáˆ‰áŠ¥áŠá‰µáŠ“ áŠ¥áˆ­áŠ«á‰³ á‹­áˆ°áˆ›á‰½áˆ á‹¨áŠ áˆáŠ•áŠá‰µ áˆ€á‹­áˆ áˆˆáˆ˜áŠ•áˆáˆ³á‹Š á‹¨áŠ¥á‹á‰€á‰µ á‰¥áˆ­áˆ€áŠ• áˆ˜áˆ˜áˆªá‹« á‰ áŠ¤áŠ­áˆ€áˆ­á‰µ á‰¶áˆŒá‹¨á‰°á‹˜áŒ‹áŒ€ áˆ˜á…áˆ€á\n",
      "4. áŠ®áŠ•á‹¶áˆšáŠ’á‹¨áˆ áŠ¨áˆ†áŠ áŠ¥áˆ˜áˆáˆ³áˆˆáˆ áŠ«áˆáˆ†áŠ áˆ˜áˆáˆ´áŠ• áŠ áˆ‹á‰£áŠ­áŠ•áˆ               \n",
      "5. áˆµáŠ•á‰³á‹¨áˆáˆ áŠ¤áˆá‹«áˆµ á“áˆ­áˆ‹áˆ› áˆˆáˆ˜áŒá‰£á‰µ á‹«áŠ•áˆ³á‰¸á‹‹áˆ á‹°áŒáˆ áˆáˆˆá‰µ áˆ°á‹ á‹­á‹ á“áˆ­á‰² á‹­áˆ˜áˆ°áˆ¨á‰³áˆ á‹«áˆˆáˆ… áˆ›áŠá‹ áˆ˜á‰¸áˆ á‰ áŠ¥áˆµáŠ­áŠ•á‹µáˆ­ á‰ á‹¨áˆáŠ­áŠ’á‹«á‰± á‹±áˆ‹ á‹¨áˆ›á‹«áŠáˆ³ á‹¨áˆˆáˆ á‰ áˆ˜áˆ­áˆ… á‹¨áˆšáˆ˜áˆ« á…áŠ‘ áˆ°á‹ á‰¢áŠ–áˆ­ áŠ áˆá‰†áˆ áŠ áˆ‹áˆµá‰€áˆáŒ¥ áŠ áˆ‹á‰½áˆá‰µ\n",
      "6. áˆˆáŠ«áˆµ áŒ­á‰¥áˆ áˆˆá‰¥áˆ°áˆ½ á‹¨á•áˆ®áŒáˆ°áˆ­áŠ• áá‰¶ áˆˆáŒ¥áˆáŠ­ áŠ¥áˆáˆ á‹«áˆáŠ­ á‰£á‹³ áŠáŠ­ áŠ¥áˆáˆ­    \n",
      "7. áŠ áˆœáˆªáŠ« áˆ±áˆŒá‹­áˆ›áŠ’áŠ• á‰ áˆ˜áŒá‹°áˆ á‰€á‹­ áˆ˜áˆµáˆ˜áˆ©áŠ• áˆ³á‰³áˆá áŠ á‹­á‰€áˆ­áˆ á‹œáŒá‰¿ áŠ áŒˆáˆªá‰±áŠ• áˆˆá‰€á‹ áŠ¥áŠ•á‹²á‹ˆáŒ¡ áŠ á‹á‹›áˆˆá‰½ áŒ€áˆˆáˆ¶á‰¿ á‹°áŒáˆ áŠ¥á‹«áˆ‰ áˆ˜áˆˆáˆ›áˆ˜áŒ¥ áŒ€áˆáˆ¨á‹‹áˆ\n",
      "8. á‹­áˆ„á‹ áŠ á‹­á‹°áˆ á‹¨áŠ¥á‹á‰€á‰µáˆ½ áŒ¥áŒá‰ áˆ°áˆš áˆ°áˆš áŠ¨áˆá‰µáŠ“áŒˆáˆª á‰³áˆªáŠ­ áŠ á‰³áŠá‰¢áˆá‹°áˆ áˆ«áˆµáˆ½áŠ• áŠ á‰³áˆµáŒˆáˆá‰º\n",
      "9. á‹¨áˆšá‰£áŠ­áŠ• áˆ˜áŠ–áˆ­ á‹¨áˆˆá‰ á‰µáˆ á‹³á‹­ áˆµáˆ«                             \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 10 cleaned tweets:\")\n",
    "for index, tweet in enumerate(data['cleaned_tweet'].head(10)):\n",
    "    print(f\"{index}. {tweet.ljust(50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data['sentiment'] = data['sentiment'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2327b1d93674ac38bb3d57f405226f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a588592584ebdaddc3f5947ff9bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496d447a506340028342a9fbdac872c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d5b207ba1e4e7885032c98ea7e73af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Davlan/afro-xlmr-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–', 'áˆ˜áŒ¥', 'áˆ‹á‰µ', 'â–áˆ˜áˆáŠ«áˆ', 'â–áŠ áˆµá‰°áˆ³áˆ°á‰¥', 'â–áŠ á‹­á‹°áˆˆáˆá¢']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"áˆ˜áŒ¥áˆ‹á‰µ áˆ˜áˆáŠ«áˆ áŠ áˆµá‰°áˆ³áˆ°á‰¥ áŠ á‹­á‹°áˆˆáˆá¢\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(samples):\n",
    "  tokenized_samples = tokenizer(samples[\"clean_tweet\"], truncation=True, max_length=512)\n",
    "  return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
