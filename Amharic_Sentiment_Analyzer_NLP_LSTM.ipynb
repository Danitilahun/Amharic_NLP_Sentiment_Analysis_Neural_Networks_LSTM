{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tiled\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.0/9.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------------------------------------- 0.1/9.7 MB 491.5 kB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/9.7 MB 585.1 kB/s eta 0:00:17\n",
      "    --------------------------------------- 0.2/9.7 MB 654.6 kB/s eta 0:00:15\n",
      "    --------------------------------------- 0.2/9.7 MB 860.2 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/9.7 MB 951.8 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.5/9.7 MB 1.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/9.7 MB 1.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/9.7 MB 1.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/9.7 MB 1.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/9.7 MB 2.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/9.7 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.5/9.7 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.7 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.2/9.7 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.3/9.7 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 450.7/450.7 kB 29.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 303.8/303.8 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 43.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 38.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.3\n",
      "    Uninstalling huggingface-hub-0.23.3:\n",
      "      Successfully uninstalled huggingface-hub-0.23.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --default-timeout=100 pandas transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Abbreviations and Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), 'data'))\n",
    "\n",
    "import amharic_preprocessing_data\n",
    "\n",
    "abbreviations = amharic_preprocessing_data.abbreviations_dictionary_data\n",
    "\n",
    "stopwords = amharic_preprocessing_data.amharic_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ዶ/ር: ዶክተር\n",
      "ዶር: ዶክተር\n",
      "ጠ/ሚ: ጠቅላይ ሚኒስቴር\n",
      "ጠ/ሚኒስትር: ጠቅላይ ሚኒስትር\n",
      "ጠ/ሚኒስትርነት: ጠቅላይ ሚኒስትርነት\n",
      "ጠ/ፍ/ቤት: ጠቅላይ ፍርድ ቤት\n",
      "ፌ/ም/ቤት: ፌደራል ምክር ቤት\n",
      "ፍ/ቤቱ: ፍርድ ቤቱ\n",
      "ፍ/ቤት: ፍርድ ቤት\n",
      "ፕ/ት: ፕሬዚዳንት\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(abbreviations.items())[:10]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Sample Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ሰሞን', 'ወዲያ', 'ጋር', 'የእሷ', 'ወይስ', 'እናንተ', 'እኔ', 'እየኖርኩ', 'እንዴት', 'ት']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ልዩ የተፈጥሮ ገፅታ *****የምስራቅ አፍሪካ የውሃ ማማ ጮቄ            \n",
      "@bobomaheder Global citizenንነቴ እና ኢትዮጵያዊነቴ ተምታተውብኝ አያውቁም የሚል ይታከልበት 😉\n",
      "RT @TechinEthiopia: ሐሴትን ከበጎነት !!! የሰው ልጅ በምድር ሲመላለስ ታላቅ የመንፈስ እርካታን ከሚያጎናፅፉት ተግባራት መካከል አንዱ ለተቸገረ መድህን፤ ለወገንም አለኝታ መሆን ሲችል እንደሆነ ብዙዎቻችንን…\n",
      "ለቅዳሚታችሁ 💚💛❤ አሁን በምትሰሩት ማንኛውም ነገር ምሉእነትና እርካታ ይሰማችሁ! የአሁንነት ሀይል -ለመንፈሳዊ የእውቀት ብርሃን መመሪያ በኤክሀርት ቶሌየተዘጋጀ መጽሐፍ ነው። https://t.co/RTi8kKKrd0 https://t.co/4YTikE0Amc\n",
      "@tesfamaryam21 40/60 ኮንዶሚኒየም ከሆነ እመልሳለሁ ካልሆነ መልሴን አላባክንም!\n",
      "@Jeberara1 ስንታየሁም ሆነ ኤልያስ ፓርላማ ለመግባት ምን ያንሳቸዋል??? ደግሞ ሁለት ሰው ብቻ ይዞ ፓርቲ ይመሰረታል ያለህ ማነው? መቸም በእስክንድር ላይ በየምክኒያቱ ዱላ የማያነሳ የለም?! በመርህ የሚመራ አንድ ፅኑ ሰው ቢኖር አልቆም አላስቀምጥ አላችሁት።\n",
      "@woldeyes_t Tesfaye ለካስ ጭብል ለብሰሽ የፕሮፌሰርን ፎቶ ለጥፈክ እልም ያልክ ባዳ ነክ እፈር ትንሽ\n",
      "አሜሪካ ሱሌይማኒን በመግደል ቀይ መስመሩን ሳታልፍ አይቀርም። ዜጎቿ አገሪቱን ለቀው እንዲወጡ አዝዛለች። ጀለሶቿ ደግሞ restrain እና de-escalate እያሉ መለማመጥ ጀምረዋል።\n",
      "@haset_haset7 ይሄው ነው አይደል የእውቀትሽ ጥግ....በሰሚ ሰሚ ከምትናገሪ ለምን ታሪክ አታነቢም....ደሞ ራስሽን አታስገምቺ\n",
      "@BirrkanF @abrshel @melak_abera የሚባክን ጊዜ መኖር የለበትም።\n",
      "ዳይ ወደ ሥራ! 😜\n",
      "\n",
      " Number of rows in the dataset: 16722\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join('data', 'dataset.csv'))\n",
    "\n",
    "for tweet in data['tweet'].head(10):\n",
    "    print(tweet.ljust(50))\n",
    "\n",
    "rows, columns = data.shape\n",
    "\n",
    "print(f\"\\n Number of rows in the dataset: {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Character-Level Mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char_level_missmatch(input_token):\n",
    "    \"\"\"\n",
    "    Normalizes character-level mismatches in Amharic text.\n",
    "\n",
    "    Parameters:\n",
    "        input_token (str): The Amharic text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    rep1 = re.sub('[ሃኅኃሐሓኻ]', 'ሀ', input_token)\n",
    "    rep2 = re.sub('[ሑኁዅ]', 'ሁ', rep1)\n",
    "    rep3 = re.sub('[ኂሒኺ]', 'ሂ', rep2)\n",
    "    rep4 = re.sub('[ኌሔዄ]', 'ሄ', rep3)\n",
    "    rep5 = re.sub('[ሕኅ]', 'ህ', rep4)\n",
    "    rep6 = re.sub('[ኆሖኾ]', 'ሆ', rep5)\n",
    "    rep7 = re.sub('[ሠ]', 'ሰ', rep6)\n",
    "    rep8 = re.sub('[ሡ]', 'ሱ', rep7)\n",
    "    rep9 = re.sub('[ሢ]', 'ሲ', rep8)\n",
    "    rep10 = re.sub('[ሣ]', 'ሳ', rep9)\n",
    "    rep11 = re.sub('[ሤ]', 'ሴ', rep10)\n",
    "    rep12 = re.sub('[ሥ]', 'ስ', rep11)\n",
    "    rep13 = re.sub('[ሦ]', 'ሶ', rep12)\n",
    "    rep14 = re.sub('[ዓኣዐ]', 'አ', rep13)\n",
    "    rep15 = re.sub('[ዑ]', 'ኡ', rep14)\n",
    "    rep16 = re.sub('[ዒ]', 'ኢ', rep15)\n",
    "    rep17 = re.sub('[ዔ]', 'ኤ', rep16)\n",
    "    rep18 = re.sub('[ዕ]', 'እ', rep17)\n",
    "    rep19 = re.sub('[ዖ]', 'ኦ', rep18)\n",
    "    rep20 = re.sub('[ጸ]', 'ፀ', rep19)\n",
    "    rep21 = re.sub('[ጹ]', 'ፁ', rep20)\n",
    "    rep22 = re.sub('[ጺ]', 'ፂ', rep21)\n",
    "    rep23 = re.sub('[ጻ]', 'ፃ', rep22)\n",
    "    rep24 = re.sub('[ጼ]', 'ፄ', rep23)\n",
    "    rep25 = re.sub('[ጽ]', 'ፅ', rep24)\n",
    "    rep26 = re.sub('[ጾ]', 'ፆ', rep25)\n",
    "    rep27 = re.sub('(ሉ[ዋአ])', 'ሏ', rep26)\n",
    "    rep28 = re.sub('(ሙ[ዋአ])', 'ሟ', rep27)\n",
    "    rep29 = re.sub('(ቱ[ዋአ])', 'ቷ', rep28)\n",
    "    rep30 = re.sub('(ሩ[ዋአ])', 'ሯ', rep29)\n",
    "    rep31 = re.sub('(ሱ[ዋአ])', 'ሷ', rep30)\n",
    "    rep32 = re.sub('(ሹ[ዋአ])', 'ሿ', rep31)\n",
    "    rep33 = re.sub('(ቁ[ዋአ])', 'ቋ', rep32)\n",
    "    rep34 = re.sub('(ቡ[ዋአ])', 'ቧ', rep33)\n",
    "    rep35 = re.sub('(ቹ[ዋአ])', 'ቿ', rep34)\n",
    "    rep36 = re.sub('(ሁ[ዋአ])', 'ኋ', rep35)\n",
    "    rep37 = re.sub('(ኑ[ዋአ])', 'ኗ', rep36)\n",
    "    rep38 = re.sub('(ኙ[ዋአ])', 'ኟ', rep37)\n",
    "    rep39 = re.sub('(ኩ[ዋአ])', 'ኳ', rep38)\n",
    "    rep40 = re.sub('(ዙ[ዋአ])', 'ዟ', rep39)\n",
    "    rep41 = re.sub('(ጉ[ዋአ])', 'ጓ', rep40)\n",
    "    rep42 = re.sub('(ደ[ዋአ])', 'ዷ', rep41)\n",
    "    rep43 = re.sub('(ጡ[ዋአ])', 'ጧ', rep42)\n",
    "    rep44 = re.sub('(ጩ[ዋአ])', 'ጯ', rep43)\n",
    "    rep45 = re.sub('(ጹ[ዋአ])', 'ጿ', rep44)\n",
    "    rep46 = re.sub('(ፉ[ዋአ])', 'ፏ', rep45)\n",
    "    rep47 = re.sub('[ቊ]', 'ቁ', rep46)\n",
    "    rep48 = re.sub('[ኵ]', 'ኩ', rep47)\n",
    "    return rep48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Short Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_short_form(input_short_word):\n",
    "    \"\"\"\n",
    "    Expands abbreviations in Amharic text using a provided dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        input_short_word (str): The word to expand.\n",
    "        short_form_dict (dict): A dictionary with abbreviations as keys and their expansions as values.\n",
    "\n",
    "    Returns:\n",
    "        str: The expanded word if found in the dictionary, otherwise the original word.\n",
    "    \"\"\"\n",
    "    return abbreviations.get(input_short_word, input_short_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english(text):\n",
    "    \"\"\"\n",
    "    Removes English characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with English characters removed.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'[a-zA-Z]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_and_special_chars(text): \n",
    "    \"\"\"\n",
    "    Removes punctuation and special characters from a given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with punctuation and special characters removed.\n",
    "    \"\"\"\n",
    "    normalized_text = re.sub(r'[!@#\\$%\\^\\«\\»&\\*\\(\\)…\\[\\]\\{\\};“”›’‘\"\\':,.\\‹/\\<\\>\\?\\\\|\\`\\´~\\-=+\\፡።፤;፦፥፧፨፠፣_]', '', text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing ASCII Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ascii_and_numbers(text_input):\n",
    "    \"\"\"\n",
    "    Removes all ASCII characters, English numbers, and Amharic/Arabic numbers from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text_input (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with ASCII characters and numbers removed.\n",
    "    \n",
    "    Notes:\n",
    "        - ASCII characters include all English letters (A-Z, a-z) and digits (0-9).\n",
    "        - Amharic/Arabic numbers are removed based on their Unicode range (U+1369 to U+137C).\n",
    "        - This function is helpful for preprocessing text where only specific non-numeric and non-ASCII \n",
    "          characters (e.g., Amharic script) are required.\n",
    "    \"\"\"\n",
    "    rm_num_and_ascii = re.sub('[A-Za-z0-9]', '', text_input)\n",
    "    \n",
    "    cleaned_text = re.sub('[\\u1369-\\u137C]+', '', rm_num_and_ascii)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace from the start and end of the given text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The trimmed text with no leading or trailing whitespace.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Removes URLs, including shortened links like 'https://t.co/...', from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to process.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with URLs removed.\n",
    "    \"\"\"\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    \n",
    "    cleaned_text = re.sub(url_pattern, '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from the text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "        stopwords (list): A list of stopwords.\n",
    "\n",
    "    Returns:\n",
    "        str: The text without stopwords.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Removes emojis from the input text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with emojis removed.\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # emotions\n",
    "                                 \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                 \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                 \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                 \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                 \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                 \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                 \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                 \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                                 \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Amharic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_amharic_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the given Amharic text by applying various preprocessing steps.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Amharic text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text after applying all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    text = remove_english(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_ascii_and_numbers(text)\n",
    "    text = remove_punc_and_special_chars(text)\n",
    "    text = normalize_char_level_missmatch(text)\n",
    "    text = expand_short_form(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Cleaning Function to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweet'] = data['tweet'].apply(clean_amharic_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 cleaned tweets:\n",
      "0. ልዩ የተፈጥሮ ገፅታ የምስራቅ አፍሪካ የውሀ ማማ ጮቄ                 \n",
      "1. ኢትዮጵያዊነቴ ተምታተውብኝ አያውቁም የሚል ይታከልበት                 \n",
      "2. ሀሴትን ከበጎነት የሰው ልጅ በምድር ሲመላለስ ታላቅ የመንፈስ እርካታን ከሚያጎናፅፉት ተግባራት አንዱ ለተቸገረ መድህን ለወገንም አለኝታ መሆን ሲችል ብዙዎቻችንን\n",
      "3. ለቅዳሚታችሁ በምትሰሩት ማንኛውም ነገር ምሉእነትና እርካታ ይሰማችሁ የአሁንነት ሀይል ለመንፈሳዊ የእውቀት ብርሀን መመሪያ በኤክሀርት ቶሌየተዘጋጀ መፅሀፍ\n",
      "4. ኮንዶሚኒየም ከሆነ እመልሳለሁ ካልሆነ መልሴን አላባክንም               \n",
      "5. ስንታየሁም ኤልያስ ፓርላማ ለመግባት ያንሳቸዋል ደግሞ ሁለት ሰው ይዞ ፓርቲ ይመሰረታል ያለህ ማነው መቸም በእስክንድር በየምክኒያቱ ዱላ የማያነሳ የለም በመርህ የሚመራ ፅኑ ሰው ቢኖር አልቆም አላስቀምጥ አላችሁት\n",
      "6. ለካስ ጭብል ለብሰሽ የፕሮፌሰርን ፎቶ ለጥፈክ እልም ያልክ ባዳ ነክ እፈር    \n",
      "7. አሜሪካ ሱሌይማኒን በመግደል ቀይ መስመሩን ሳታልፍ አይቀርም ዜጎቿ አገሪቱን ለቀው እንዲወጡ አዝዛለች ጀለሶቿ ደግሞ እያሉ መለማመጥ ጀምረዋል\n",
      "8. ይሄው አይደል የእውቀትሽ ጥግበሰሚ ሰሚ ከምትናገሪ ታሪክ አታነቢምደሞ ራስሽን አታስገምቺ\n",
      "9. የሚባክን መኖር የለበትም ዳይ ስራ                             \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 10 cleaned tweets:\")\n",
    "for index, tweet in enumerate(data['cleaned_tweet'].head(10)):\n",
    "    print(f\"{index}. {tweet.ljust(50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data['sentiment'] = data['sentiment'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2327b1d93674ac38bb3d57f405226f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9a588592584ebdaddc3f5947ff9bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496d447a506340028342a9fbdac872c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d5b207ba1e4e7885032c98ea7e73af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"Davlan/afro-xlmr-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'መጥ', 'ላት', '▁መልካም', '▁አስተሳሰብ', '▁አይደለም።']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"መጥላት መልካም አስተሳሰብ አይደለም።\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(samples):\n",
    "  tokenized_samples = tokenizer(samples[\"clean_tweet\"], truncation=True, max_length=512)\n",
    "  return tokenized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
